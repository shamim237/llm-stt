{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('''\n",
        "<iframe width=\"720\" height=\"380\" src=\"https://www.youtube.com/embed/qCNSSAzWx8U\" frameborder=\"0\" allowfullscreen></iframe>\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "S3Ew_7OhfHsS",
        "outputId": "e3510f2d-1243-4151-f973-18fbc2c35690"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"720\" height=\"380\" src=\"https://www.youtube.com/embed/qCNSSAzWx8U\" frameborder=\"0\" allowfullscreen></iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtLyd04ogb9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8hQ--mbgcFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:** To minimize hallucinations of a language model resulting in false or wrong information hence improving precisions and reliability of LLM outcomes respective of various techniques.\n",
        "\n",
        "**Understanding Hallucinations:**\n",
        "\n",
        "*  **Cause Analysis:** Hallucinations arise when LLMs on a confidence basis produce wrong information, which is why there are so many of them. This happens because the LLMs highly depend on their previous datasets that might be obsolete at times or not comprehensive enough. LLMs generate content that appears to be for instance right according to this information but they cannot verify anything about its truthfulness nor do they have access to latest updates. Additionally, when overtraining occurs, LLMs will become stickier to their previous databases making it harder for them to give fresh and original answers all the time. In situations where data is inadequate, the model could provide responses derived from previous encounters which may be misleading. It’s crucial to understand that LLMs produce responses by following trends hence do not possess moral or authoritative reasoning whatsoever.\n",
        "Another contributor causing hallucinations is overfitting; this leads to very close resemblance of some LLMs with their training dataset such that generation of entirely new and original content becomes next to impossible. The other point to note is that when the LLM requires more knowledge for making a statement than it has acquired, it may produce a response that has been derived from past occurrences. It is also necessary to point out that LLMs cannot verify any information they might have received. They build solutions based on\n",
        "regularities but not ethical or true evaluations.\n",
        "\n",
        "*  **Solutions:**\n",
        "Several strategies can help ensure LLMs generate accurate responses.\n",
        "\n",
        " *    **Context Injection & Advanced Prompt Engineering:** Adding more information to the prompt, known as \"context injection,\" can enhance LLM performance. This involves providing additional text, code, or data to ensure the LLM has sufficient context. For example, feeding the model relevant examples can help it produce higher-quality and more accurate content. Properly constructed prompts, combined with techniques like grounding (retrieving external data sources) and dynamic prompt generation, can reduce hallucinations. By leveraging real-time information from external sources like documents  databases, grounding ensures that LLMs don’t rely solely on their training data.\n",
        "In addition to enhancing the input with more descriptive and clarifying aspects, prompt augmentation may lead to the generation of better results. As an example, a chatbot embedded in a retail website may present users’ inquiries automatically enhanced with specific information about products and hence make the model’s reply better.\n",
        "\n",
        " *   **Retrieval-Augmented Generation (RAG) with Vector Databases:** The RAG model is a powerful tool for combating hallucinations when combined with vector databases. This means that before creating a response LLMs can look for the right information using outside knowledge, like searching the internet or getting documents. Therefore, they reduce chances of hallucination by using real-time data from authentic sources to generate answers. Vector databases enhance this further because they store text as numeric vectors (embeddings) which denote meaning within the text itself. In addition, queries are also transformed into vectors making it possible for the database to find pertinent documents even if these documents do not have the same terms.\n",
        "\n",
        "Moreover, developers can modify how the model behaves by changing some parameters such as “temperature”, that reduces its creativity and thus focuses more on generating accurate answers. Other approaches include prompt engineering and context injection which make LLMs more precise and reliable without having to retrain or fine-tune them all over again. When implemented together with RAG , this approach helps improve LLM performance in generating relevant fact-based outputs while at the same time being scalable and affordable.\n"
      ],
      "metadata": {
        "id": "Q34Tn2TYM_cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jl--pQn0oeb",
        "outputId": "80ed85af-d46c-41d4-e04d-2ed9c538e688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Example of LLM Hallucination***"
      ],
      "metadata": {
        "id": "qmMKaVsQCMpI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obek0iNV0inz",
        "outputId": "51c50403-a32e-43f0-90fa-0900844b872f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What happened to KFC Company on 25th of July, 2015?\n",
            "Hallucinated Response: On July 25, 2015, KFC faced a shortage of chicken in many of its restaurants across the United Kingdom. The shortage was a result of operational issues with their new delivery partner, DHL, which led to a disruption in the supply chain. As a result, many KFC restaurants had to close temporarily or operate with a limited menu until the issue was resolved. This incident caused significant inconvenience to customers and drew media attention at the time.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# OpenAI API key\n",
        "openai.api_key = '**************************'\n",
        "\n",
        "def get_response(prompt):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        # Accessing the response data correctly\n",
        "        return response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# A prompt that could lead to a hallucination\n",
        "prompt = 'What happened to KFC Company on 25th of July, 2015?'\n",
        "\n",
        "response = get_response(prompt)\n",
        "\n",
        "print(f\"Question: {prompt}\")\n",
        "print(f\"Hallucinated Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***solution using prompt engineering***"
      ],
      "metadata": {
        "id": "P0tGm_irCVcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt):\n",
        "    try:\n",
        "        # Modify system prompt to encourage creativity instead of accuracy\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"If no reliable source is found for a query, say 'I don't have that information' rather than guessing.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        # Accessing the response data correctly\n",
        "        return response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# A prompt that could lead to a hallucination\n",
        "prompt = 'What happened to KFC Company on 25th of July, 2015?'\n",
        "\n",
        "response = get_response(prompt)\n",
        "\n",
        "print(f\"Question: {prompt}\")\n",
        "print(f\"Improved Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_EYZcTG1m7K",
        "outputId": "03f6c733-b2cc-424d-df13-b1480d57cf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What happened to KFC Company on 25th of July, 2015?\n",
            "Improved Response: I don't have that information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***solution using RAG***"
      ],
      "metadata": {
        "id": "87BW5AjJCHIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "TOe02BoB3gfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dummy database of documents for retrieval (for illustration)\n",
        "documents = {\n",
        "    \"doc1\": \"On 19 February 2018, KFC has closed more than half of its 900 UK outlets after delivery problems meant they ran out of chicken.\",\n",
        "    \"doc2\": \"KFC is launching an unexpected new product: fried-chicken scented sunscreen on Aug 22, 2016, but nothing significant occurred on July 25th.\",\n",
        "    \"doc3\": \"There is no known information about KFC on July 25th, 2015.\",\n",
        "}\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"Get the embedding of a given text using OpenAI's embedding model.\"\"\"\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return response['data'][0]['embedding']\n",
        "\n",
        "def retrieve_relevant_documents(query, documents):\n",
        "    \"\"\"Retrieve the most relevant document by comparing embeddings.\"\"\"\n",
        "    query_embedding = embed_text(query)\n",
        "    document_embeddings = {doc_id: embed_text(text) for doc_id, text in documents.items()}\n",
        "\n",
        "    # Compute cosine similarity between query and documents\n",
        "    similarities = {doc_id: cosine_similarity([query_embedding], [embedding])[0][0]\n",
        "                    for doc_id, embedding in document_embeddings.items()}\n",
        "\n",
        "    # Sort documents by similarity score\n",
        "    ranked_documents = sorted(similarities, key=similarities.get, reverse=True)\n",
        "\n",
        "    # Return the most relevant document\n",
        "    return ranked_documents[0]\n",
        "\n",
        "def get_response_with_rag(prompt):\n",
        "    try:\n",
        "        # Step 1: Retrieve relevant document based on the query\n",
        "        relevant_doc_id = retrieve_relevant_documents(prompt, documents)\n",
        "        relevant_document = documents[relevant_doc_id]\n",
        "\n",
        "        # Step 2: Pass the relevant document as context to the language model\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a factual assistant. Use the following document to assist in your answer:\"},\n",
        "                {\"role\": \"system\", \"content\": relevant_document},  # Retrieved document as context\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "        return response.choices[0].message['content']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example prompt that previously led to hallucination\n",
        "prompt = \"What happened to KFC Company on 25th of July, 2015?\"\n",
        "\n",
        "response = get_response_with_rag(prompt)\n",
        "print(f\"Question: {prompt}\")\n",
        "print(f\"Improved Response: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTdP5ugl007k",
        "outputId": "c089b820-c00f-4a53-d6e6-6d1a90675695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What happened to KFC Company on 25th of July, 2015?\n",
            "Improved Response: There is no specific information available about any significant event or incident related to KFC on July 25th, 2015. If you have any other questions or need information on a different date, feel free to ask.\n"
          ]
        }
      ]
    }
  ]
}